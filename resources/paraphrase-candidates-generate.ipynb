{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Candidate Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate **non-academic** to **academic** word pairs for paaphrasing, we used the paraphrase (word-pairs) in CoInCo, WordNet and PPDB as the starting point.\n",
    "\n",
    "For the CoInCo dataset, we have included only those word pairs where : 1) the target word is non-academic, 2) the substitution candidate is academic, 3) the target word has a higher word frequency than the substitute candidate in our academic resources. Since the academic resource is not exhaustive, some proper academic terms may be mistakenly considered as **non-academic**.\n",
    "\n",
    "We have collected a total of 23,476 word pairs from the CoInCo training set. The dataset is prepared with 4 candidates for each informal target, where 2 candidates are academic and 2 are non-academic. When we do not have appropriate candidates we extract further candidates from WordNet and PPDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, pickle, re, pickle\n",
    "from collections import Counter\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # to replicate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoInCo = '<path-to-coinco.xml>'\n",
    "COCA_ALL = 'path-to-(COCA)allWords.xlsx>'\n",
    "COMPILED_LIST = '<path-to-academic_keyphrases.xlsx>'\n",
    "COCA_LIST = '<path-to-(COCA)acadCore.xlsx>'\n",
    "NAWL = '<path-to-NAWL_Headwords.txt>'\n",
    "ACL_FREQ = '<path-to-academic_unigrams.pkl>' # obained while compiling the resources\n",
    "BEAUTIFUL_DATA = '<path-to-(beautiful_data)count_1w.txt>'\n",
    "GLOVE_PATH = 'path-to-glove.840B.300d.txt'\n",
    "\n",
    "PRECONTEXT = 0\n",
    "TARGETSENTENCE = 1\n",
    "POSTCONTEXT = 2\n",
    "TOKENS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath(GLOVE_PATH)\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(CoInCo)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "for child in root:\n",
    "    sentences.append(child[TARGETSENTENCE].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.Random(9).shuffle(sentences)\n",
    "train_sentences = sentences[ : int(0.65 * len(sentences))]\n",
    "test_sentences = sentences[int(0.65 * len(sentences)) : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_sentences), len(test_sentences), len(train_sentences)+len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sentences = train_sentences[ : int(0.8 * len(train_sentences))]\n",
    "v_sentences = train_sentences[int(0.8 * len(train_sentences)) : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = dict()\n",
    "v_d = dict()\n",
    "test_d = dict()\n",
    "\n",
    "for child in root:\n",
    "    for token in child[TOKENS]:\n",
    "        token_id = token.get('id')\n",
    "        t = dict()\n",
    "        t['precontenxt'] = child[PRECONTEXT].text.strip()\n",
    "        t['postcontext'] = child[POSTCONTEXT].text.strip()\n",
    "        t['wordform'] = token.get('wordform')\n",
    "        t['lemma'] = token.get('lemma')\n",
    "        t['posMASC'] = token.get('posMASC')\n",
    "        t['posTT'] = token.get('posTT')\n",
    "        t['problematic'] = token.get('problematic')\n",
    "        l = list()\n",
    "        for substitutions in token:\n",
    "            for subst in substitutions:\n",
    "                s = (subst.get('lemma'), subst.get('pos'), subst.get('freq'))\n",
    "                l.append(s)\n",
    "        t['substitutions'] = l\n",
    "        \n",
    "        if(token_id != 'XXX' and (child[TARGETSENTENCE].text.strip() in t_sentences)):\n",
    "            t['targetsentence'] = child[TARGETSENTENCE].text.strip()\n",
    "            t_d[token_id] = t\n",
    "        elif(token_id != 'XXX' and (child[TARGETSENTENCE].text.strip() in v_sentences)):\n",
    "            t['targetsentence'] = child[TARGETSENTENCE].text.strip()\n",
    "            v_d[token_id] = t\n",
    "        elif(token_id != 'XXX' and (child[TARGETSENTENCE].text.strip() in test_sentences)):\n",
    "            t['targetsentence'] = child[TARGETSENTENCE].text.strip()\n",
    "            test_d[token_id] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = dict()\n",
    "for t in t_d:\n",
    "    train_d[t] = t_d[t]\n",
    "for t in v_d:\n",
    "    train_d[t] = v_d[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_d), len(v_d), len(train_d), len(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "academic_df = pd.read_excel(COMPILED_LIST, sheet_name='<sheet-name>')\n",
    "academic_list = academic_df.phrase.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_df = pd.read_excel(COCA_LIST, sheet_name='list')\n",
    "coca_list = coca_df.word.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NAWL, 'r') as f:\n",
    "    s = f.read()\n",
    "    nawl_list = s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ACL_FREQ, 'rb') as f:\n",
    "    acl_freq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatiful_data_freq = Counter()\n",
    "with open(BEAUTIFUL_DATA, 'r') as f:\n",
    "    tmp = f.read().strip().split('\\n')\n",
    "    for c in tmp:\n",
    "        word, freq = c.strip().split('\\t')\n",
    "        beatiful_data_freq[word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords_df = pd.read_excel(COCA_ALL, sheet_name='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class DBS:\n",
    "    def __init__(self, host, username, password):\n",
    "        self.host = host\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "    def get_ppdb2_candidates(self, phrase):\n",
    "        db = MySQLdb.connect(self.host, self.username, self.password, \"ppdb2\")\n",
    "        cursor = db.cursor()\n",
    "        cmd = \"select target, ppdb2score from ppdb where source = '%s' order by ppdb2score desc\" % (str(phrase))\n",
    "        cursor.execute(cmd)\n",
    "        results = cursor.fetchall()\n",
    "        return results\n",
    "\n",
    "    def get_wordnet_candidates(self, word):\n",
    "        word = word.replace(' ', '_')\n",
    "        synonyms = list()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "        return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBS('ltdatabase1', 'dummy', 'dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_substitute(sentence, target, candidate):\n",
    "    return sentence.replace(target, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_repeat(candidate, subs):\n",
    "    for s in subs:\n",
    "        if(candidate == s[0]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_embed = np.random.rand(300,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        return UNK_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence):\n",
    "    # remove special characters\n",
    "    sentence = ' '.join(re.findall(r\"[a-zA-Z0-9]+\", sentence))\n",
    "\n",
    "    words_embed = list()\n",
    "    word_list = sentence.split()\n",
    "    for word in word_list:\n",
    "        words_embed.append(get_word_embedding(word))\n",
    "    \n",
    "    return np.mean(words_embed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2index = {\n",
    "    'freq_beatiful' : 1,\n",
    "    'freq_coca_general' : 2,\n",
    "    'freq_acl' : 3,\n",
    "    'cos_target' : 4,\n",
    "    'euclidean_distance' : 5,\n",
    "    'posMASC_le' : 6,\n",
    "    'is_problematic' : 7,\n",
    "    'word_length' : 8,\n",
    "    'count_vowel' : 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs_t = dict()\n",
    "for token_id in tqdm_notebook(t_d):\n",
    "    lemma = t_d[token_id]['lemma']\n",
    "    wordform = t_d[token_id]['wordform']\n",
    "    sentence_embed = get_sentence_embedding(t_d[token_id]['targetsentence'])\n",
    "    if(lemma not in coca_list or lemma not in nawl_list or lemma not in academic_list):\n",
    "        academic_subs = list()\n",
    "        non_academic_subs = list()\n",
    "        for subst in t_d[token_id]['substitutions']:\n",
    "            s = subst[0]\n",
    "            if(s in coca_list or s in nawl_list or s in academic_list):\n",
    "                try:\n",
    "                    if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == s].iloc[0]['COCA-All']):\n",
    "                        l = list(subst)\n",
    "                        pos = l.pop(1)\n",
    "                        l.append('0')\n",
    "                        l.append(pos)\n",
    "                        word_embed = get_word_embedding(s)\n",
    "                        cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                        l.append(cos_sim)\n",
    "                        l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                        if(len(academic_subs) < 2):\n",
    "                            academic_subs.append(l)\n",
    "                except:\n",
    "                    l = list(subst)\n",
    "                    pos = l.pop(1)\n",
    "                    l.append('1')\n",
    "                    l.append(pos)\n",
    "                    word_embed = get_word_embedding(s)\n",
    "                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                    l.append(cos_sim)\n",
    "                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                    if(len(non_academic_subs) < 2):\n",
    "                        non_academic_subs.append(l)\n",
    "            else:\n",
    "                l = list(subst)\n",
    "                pos = l.pop(1)\n",
    "                l.append('1')\n",
    "                l.append(pos)\n",
    "                word_embed = get_word_embedding(s)\n",
    "                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                l.append(cos_sim)\n",
    "                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                if(len(non_academic_subs) < 2):\n",
    "                    non_academic_subs.append(l)\n",
    "\n",
    "        if(academic_subs):\n",
    "            if(len(academic_subs) < 2 or len(non_academic_subs) < 2):\n",
    "                wordnet_candidates = db.get_wordnet_candidates(lemma)\n",
    "                for t in wordnet_candidates:\n",
    "                    candidate = t[0]\n",
    "                    if(candidate in coca_list or candidate in nawl_list or candidate in academic_list):\n",
    "                        try:\n",
    "                            if(not is_repeat(candidate, academic_subs)):\n",
    "                                if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == candidate].iloc[0]['COCA-All']):\n",
    "                                    l = [candidate, '0', '0', 'UNK']\n",
    "                                    word_embed = get_word_embedding(candidate)\n",
    "                                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                    l.append(cos_sim)\n",
    "                                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                    if(len(academic_subs) < 2):\n",
    "                                        academic_subs.append(l)\n",
    "                        except:\n",
    "                            if(not is_repeat(candidate, non_academic_subs)):\n",
    "                                l = [candidate, '0', '0', 'UNK']\n",
    "                                word_embed = get_word_embedding(candidate)\n",
    "                                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                l.append(cos_sim)\n",
    "                                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                if(len(non_academic_subs) < 2):\n",
    "                                    non_academic_subs.append(l)\n",
    "                    else:\n",
    "                        if(not is_repeat(candidate, non_academic_subs)):\n",
    "                            l = [candidate, '0', '0', 'UNK']\n",
    "                            word_embed = get_word_embedding(candidate)\n",
    "                            cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                            l.append(cos_sim)\n",
    "                            l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                            if(len(non_academic_subs) < 2):\n",
    "                                non_academic_subs.append(l)\n",
    "            if(len(academic_subs) < 2 or len(non_academic_subs) < 2):\n",
    "                ppdb_candidates = db.get_ppdb2_candidates(lemma)\n",
    "                for t in ppdb_candidates:\n",
    "                    candidate = t[0]\n",
    "                    if(candidate in coca_list or candidate in nawl_list or candidate in academic_list):\n",
    "                        try:\n",
    "                            if(not is_repeat(candidate, academic_subs)):\n",
    "                                if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == candidate].iloc[0]['COCA-All']):\n",
    "                                    l = [candidate, '0', '0', 'UNK']\n",
    "                                    word_embed = get_word_embedding(candidate)\n",
    "                                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                    l.append(cos_sim)\n",
    "                                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                    if(len(academic_subs) < 2):\n",
    "                                        academic_subs.append(l)\n",
    "                        except:\n",
    "                            if(not is_repeat(candidate, non_academic_subs)):\n",
    "                                l = [candidate, '0', '0', 'UNK']\n",
    "                                word_embed = get_word_embedding(candidate)\n",
    "                                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                l.append(cos_sim)\n",
    "                                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                if(len(non_academic_subs) < 2):\n",
    "                                    non_academic_subs.append(l)\n",
    "                    else:\n",
    "                        if(not is_repeat(candidate, non_academic_subs)):\n",
    "                            l = [candidate, '0', '0', 'UNK']\n",
    "                            word_embed = get_word_embedding(candidate)\n",
    "                            cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                            l.append(cos_sim)\n",
    "                            l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                            if(len(non_academic_subs) < 2):\n",
    "                                non_academic_subs.append(l)\n",
    "            if(len(academic_subs) == 2 and len(non_academic_subs) == 2):\n",
    "                academic_subs = sorted(academic_subs, key=lambda x: int(x[1]), reverse=True)\n",
    "                non_academic_subs = sorted(non_academic_subs, key=lambda x: int(x[1]), reverse=True)\n",
    "                valid_subs = list()\n",
    "                valid_subs.extend(academic_subs)\n",
    "                valid_subs.extend(non_academic_subs)\n",
    "                # valid_subs : [[candidate, freq, is_non_academic, pos, cos_target, euclidean_distance], ....]\n",
    "                word_pairs_t[(token_id, wordform)] = valid_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_pairs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = list()\n",
    "for p in tqdm_notebook(word_pairs_t):\n",
    "    for candidate_info in word_pairs_t[p]:\n",
    "        pos_tags.append(candidate_info[3])\n",
    "pos_tags = set(pos_tags)\n",
    "le_pos = dict()\n",
    "for i, v in enumerate(pos_tags):\n",
    "    le_pos[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_pairs_t.pkl', 'wb') as f:\n",
    "    pickle.dump(word_pairs_t, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = 1\n",
    "doc = ''\n",
    "\n",
    "for p in tqdm_notebook(word_pairs_t):\n",
    "    for candidate_info in word_pairs_t[p]:\n",
    "\n",
    "        is_non_academic = candidate_info[2]\n",
    "        if(is_non_academic == '0'):\n",
    "            freq = candidate_info[1]\n",
    "        elif(is_non_academic == '1'):\n",
    "            freq = ('-' + candidate_info[1]) # string concate to put it into the parser\n",
    "        target_value = freq\n",
    "        \n",
    "        lemma = candidate_info[0]\n",
    "\n",
    "        try:\n",
    "            freq_beatiful = beatiful_data_freq[lemma]\n",
    "        except:\n",
    "            freq_beatiful = 0\n",
    "        f_1 = feature2index['freq_beatiful']\n",
    "        \n",
    "        try:\n",
    "            freq_coca_general = allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All']\n",
    "        except:\n",
    "            freq_coca_general = 0\n",
    "        f_2 = feature2index['freq_coca_general']    \n",
    "        \n",
    "        try:\n",
    "            freq_acl = acl_freq[tuple(lemma.split())]\n",
    "        except:\n",
    "            freq_acl = 0\n",
    "        f_3 = feature2index['freq_acl']\n",
    "        \n",
    "        cos_target = candidate_info[4]\n",
    "        f_4 = feature2index['cos_target']\n",
    "        \n",
    "        euclidean_distance = candidate_info[5]\n",
    "        f_5 = feature2index['euclidean_distance']\n",
    "        \n",
    "        posMASC = candidate_info[3]\n",
    "        posMASC_le = le_pos[posMASC]\n",
    "        f_6 = feature2index['posMASC_le']\n",
    "        \n",
    "        # is_problematic\n",
    "        \n",
    "        word_length = len(lemma)\n",
    "        f_8 = feature2index['word_length']\n",
    "        \n",
    "        count_vowel = sum(list(map(lemma.lower().count, 'aeiou')))\n",
    "        f_9 = feature2index['count_vowel']\n",
    "        \n",
    "        doc += (str(target_value) + ' qid:' + str(qid) + ' ' + str(f_1) + ':' + str(freq_beatiful) + ' ' + str(f_2) + ':' + str(freq_coca_general) + ' ' + str(f_3) + ':' + str(freq_acl) + ' ' + str(f_4) + ':' + str(cos_target) + ' ' + str(f_5) + ':' + str(euclidean_distance) + ' ' + str(f_6) + ':' + str(posMASC_le) + ' ' + str(f_8) + ':' + str(word_length) + ' ' + str(f_9) + ':' + str(count_vowel) + '\\n')\n",
    "\n",
    "    qid += 1\n",
    "\n",
    "with open('paraphrase-t.txt', 'w') as f:\n",
    "    f.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs_v = dict()\n",
    "for token_id in tqdm_notebook(v_d):\n",
    "    lemma = v_d[token_id]['lemma']\n",
    "    wordform = v_d[token_id]['wordform']\n",
    "    if(lemma not in coca_list or lemma not in nawl_list or lemma not in academic_list):\n",
    "        academic_subs = list()\n",
    "        non_academic_subs = list()\n",
    "        for subst in v_d[token_id]['substitutions']:\n",
    "            s = subst[0]\n",
    "            if(s in coca_list or s in nawl_list or s in academic_list):\n",
    "                try:\n",
    "                    if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == s].iloc[0]['COCA-All']):\n",
    "                        l = list(subst)\n",
    "                        pos = l.pop(1)\n",
    "                        l.append('0')\n",
    "                        l.append(pos)\n",
    "                        word_embed = get_word_embedding(s)\n",
    "                        cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                        l.append(cos_sim)\n",
    "                        l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                        if(len(academic_subs) < 2):\n",
    "                            academic_subs.append(l)\n",
    "                except:\n",
    "                    l = list(subst)\n",
    "                    pos = l.pop(1)\n",
    "                    l.append('1')\n",
    "                    l.append(pos)\n",
    "                    word_embed = get_word_embedding(s)\n",
    "                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                    l.append(cos_sim)\n",
    "                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                    if(len(non_academic_subs) < 2):\n",
    "                        non_academic_subs.append(l)\n",
    "            else:\n",
    "                l = list(subst)\n",
    "                pos = l.pop(1)\n",
    "                l.append('1')\n",
    "                l.append(pos)\n",
    "                word_embed = get_word_embedding(s)\n",
    "                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                l.append(cos_sim)\n",
    "                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                if(len(non_academic_subs) < 2):\n",
    "                    non_academic_subs.append(l)\n",
    "\n",
    "        if(academic_subs):\n",
    "            if(len(academic_subs) < 2 or len(non_academic_subs) < 2):\n",
    "                wordnet_candidates = db.get_wordnet_candidates(lemma)\n",
    "                for t in wordnet_candidates:\n",
    "                    candidate = t[0]\n",
    "                    if(candidate in coca_list or candidate in nawl_list or candidate in academic_list):\n",
    "                        try:\n",
    "                            if(not is_repeat(candidate, academic_subs)):\n",
    "                                if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == candidate].iloc[0]['COCA-All']):\n",
    "                                    l = [candidate, '0', '0', 'UNK']\n",
    "                                    word_embed = get_word_embedding(candidate)\n",
    "                                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                    l.append(cos_sim)\n",
    "                                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                    if(len(academic_subs) < 2):\n",
    "                                        academic_subs.append(l)\n",
    "                        except:\n",
    "                            if(not is_repeat(candidate, non_academic_subs)):\n",
    "                                l = [candidate, '0', '0', 'UNK']\n",
    "                                word_embed = get_word_embedding(candidate)\n",
    "                                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                l.append(cos_sim)\n",
    "                                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                if(len(non_academic_subs) < 2):\n",
    "                                    non_academic_subs.append(l)\n",
    "                    else:\n",
    "                        if(not is_repeat(candidate, non_academic_subs)):\n",
    "                            l = [candidate, '0', '0', 'UNK']\n",
    "                            word_embed = get_word_embedding(candidate)\n",
    "                            cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                            l.append(cos_sim)\n",
    "                            l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                            if(len(non_academic_subs) < 2):\n",
    "                                non_academic_subs.append(l)\n",
    "\n",
    "            if(len(academic_subs) < 2 or len(non_academic_subs) < 2):\n",
    "                ppdb_candidates = db.get_ppdb2_candidates(lemma)\n",
    "                for t in ppdb_candidates:\n",
    "                    candidate = t[0]\n",
    "                    if(candidate in coca_list or candidate in nawl_list or candidate in academic_list):\n",
    "                        try:\n",
    "                            if(not is_repeat(candidate, academic_subs)):\n",
    "                                if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == candidate].iloc[0]['COCA-All']):\n",
    "                                    l = [candidate, '0', '0', 'UNK']\n",
    "                                    word_embed = get_word_embedding(candidate)\n",
    "                                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                    l.append(cos_sim)\n",
    "                                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                    if(len(academic_subs) < 2):\n",
    "                                        academic_subs.append(l)\n",
    "                        except:\n",
    "                            if(not is_repeat(candidate, non_academic_subs)):\n",
    "                                l = [candidate, '0', '0', 'UNK']\n",
    "                                word_embed = get_word_embedding(candidate)\n",
    "                                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                l.append(cos_sim)\n",
    "                                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                if(len(non_academic_subs) < 2):\n",
    "                                    non_academic_subs.append(l)\n",
    "                    else:\n",
    "                        if(not is_repeat(candidate, non_academic_subs)):\n",
    "                            l = [candidate, '0', '0', 'UNK']\n",
    "                            word_embed = get_word_embedding(candidate)\n",
    "                            cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                            l.append(cos_sim)\n",
    "                            l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                            if(len(non_academic_subs) < 2):\n",
    "                                non_academic_subs.append(l)\n",
    "            if(len(academic_subs) == 2 and len(non_academic_subs) == 2):\n",
    "                academic_subs = sorted(academic_subs, key=lambda x: int(x[1]), reverse=True)\n",
    "                non_academic_subs = sorted(non_academic_subs, key=lambda x: int(x[1]), reverse=True)\n",
    "                valid_subs = list()\n",
    "                valid_subs.extend(academic_subs)\n",
    "                valid_subs.extend(non_academic_subs)\n",
    "                # valid_subs : [[candidate, freq, is_non_academic, pos, cos_target, euclidean_distance], ....]\n",
    "                word_pairs_v[(token_id, wordform)] = valid_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_pairs_v.pkl', 'wb') as f:\n",
    "    pickle.dump(word_pairs_v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = 1\n",
    "doc = ''\n",
    "\n",
    "for p in tqdm_notebook(word_pairs_v):\n",
    "    for candidate_info in word_pairs_v[p]:\n",
    "\n",
    "        is_non_academic = candidate_info[2]\n",
    "        if(is_non_academic == '0'):\n",
    "            freq = candidate_info[1]\n",
    "        elif(is_non_academic == '1'):\n",
    "            freq = ('-' + candidate_info[1]) # string concate to put it into the parser\n",
    "        target_value = freq\n",
    "        \n",
    "        lemma = candidate_info[0]\n",
    "\n",
    "        try:\n",
    "            freq_beatiful = beatiful_data_freq[lemma]\n",
    "        except:\n",
    "            freq_beatiful = 0\n",
    "        f_1 = feature2index['freq_beatiful']\n",
    "        \n",
    "        try:\n",
    "            freq_coca_general = allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All']\n",
    "        except:\n",
    "            freq_coca_general = 0\n",
    "        f_2 = feature2index['freq_coca_general']    \n",
    "        \n",
    "        try:\n",
    "            freq_acl = acl_freq[tuple(lemma.split())]\n",
    "        except:\n",
    "            freq_acl = 0\n",
    "        f_3 = feature2index['freq_acl']\n",
    "        \n",
    "        cos_target = candidate_info[4]\n",
    "        f_4 = feature2index['cos_target']\n",
    "        \n",
    "        euclidean_distance = candidate_info[5]\n",
    "        f_5 = feature2index['euclidean_distance']\n",
    "        \n",
    "        posMASC = candidate_info[3]\n",
    "        try:\n",
    "            posMASC_le = le_pos[posMASC]\n",
    "        except:\n",
    "            posMASC_le = le_pos['UNK']\n",
    "        f_6 = feature2index['posMASC_le']\n",
    "        \n",
    "        # is_problematic\n",
    "        \n",
    "        word_length = len(lemma)\n",
    "        f_8 = feature2index['word_length']\n",
    "        \n",
    "        count_vowel = sum(list(map(lemma.lower().count, 'aeiou')))\n",
    "        f_9 = feature2index['count_vowel']\n",
    "        \n",
    "        doc += (str(target_value) + ' qid:' + str(qid) + ' ' + str(f_1) + ':' + str(freq_beatiful) + ' ' + str(f_2) + ':' + str(freq_coca_general) + ' ' + str(f_3) + ':' + str(freq_acl) + ' ' + str(f_4) + ':' + str(cos_target) + ' ' + str(f_5) + ':' + str(euclidean_distance) + ' ' + str(f_6) + ':' + str(posMASC_le) + ' ' + str(f_8) + ':' + str(word_length) + ' ' + str(f_9) + ':' + str(count_vowel) + '\\n')\n",
    "\n",
    "    qid += 1\n",
    "\n",
    "with open('paraphrase-val.txt', 'w') as f:\n",
    "    f.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs_test = dict()\n",
    "no_replacement = 0\n",
    "non_academic_count = 0\n",
    "with_replacement = 0\n",
    "for token_id in tqdm_notebook(test_d):\n",
    "    lemma = test_d[token_id]['lemma']\n",
    "    wordform = test_d[token_id]['wordform']\n",
    "    sentence_embed = get_sentence_embedding(test_d[token_id]['targetsentence'])\n",
    "    if(lemma not in coca_list or lemma not in nawl_list or lemma not in academic_list):\n",
    "        non_academic_count += 1\n",
    "        academic_subs = list()\n",
    "        non_academic_subs = list()\n",
    "        c = 0\n",
    "        for subst in test_d[token_id]['substitutions']:\n",
    "            s = subst[0]\n",
    "            if(s in coca_list or s in nawl_list or s in academic_list):\n",
    "                try:\n",
    "                    if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == s].iloc[0]['COCA-All']):\n",
    "                        if(c == 0):\n",
    "                            with_replacement += 1\n",
    "                            c = 1\n",
    "                        l = list(subst)\n",
    "                        pos = l.pop(1)\n",
    "                        l.append('0')\n",
    "                        l.append(pos)\n",
    "                        word_embed = get_word_embedding(s)\n",
    "                        cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                        l.append(cos_sim)\n",
    "                        l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                        if(len(academic_subs) < 2):\n",
    "                            academic_subs.append(l)\n",
    "                except:\n",
    "                    l = list(subst)\n",
    "                    pos = l.pop(1)\n",
    "                    l.append('1')\n",
    "                    l.append(pos)\n",
    "                    word_embed = get_word_embedding(s)\n",
    "                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                    l.append(cos_sim)\n",
    "                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                    if(len(non_academic_subs) < 2):\n",
    "                        non_academic_subs.append(l)\n",
    "            else:\n",
    "                l = list(subst)\n",
    "                pos = l.pop(1)\n",
    "                l.append('1')\n",
    "                l.append(pos)\n",
    "                word_embed = get_word_embedding(s)\n",
    "                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                l.append(cos_sim)\n",
    "                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                if(len(non_academic_subs) < 2):\n",
    "                    non_academic_subs.append(l)\n",
    "\n",
    "        if(academic_subs):\n",
    "            if(len(academic_subs) < 2 or len(non_academic_subs) < 2):\n",
    "                word_candidates = db.get_wordnet_candidates(lemma)\n",
    "                for t in word_candidates:\n",
    "                    candidate = t[0]\n",
    "                    if(candidate in coca_list or candidate in nawl_list or candidate in academic_list):\n",
    "                        try:\n",
    "                            if(not is_repeat(candidate, academic_subs)):\n",
    "                                if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == candidate].iloc[0]['COCA-All']):\n",
    "                                    l = [candidate, '0', '0', 'UNK']\n",
    "                                    word_embed = get_word_embedding(candidate)\n",
    "                                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                    l.append(cos_sim)\n",
    "                                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                    if(len(academic_subs) < 2):\n",
    "                                        academic_subs.append(l)\n",
    "                        except:\n",
    "                            if(not is_repeat(candidate, non_academic_subs)):\n",
    "                                l = [candidate, '0', '0', 'UNK']\n",
    "                                word_embed = get_word_embedding(candidate)\n",
    "                                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                l.append(cos_sim)\n",
    "                                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                if(len(non_academic_subs) < 2):\n",
    "                                    non_academic_subs.append(l)\n",
    "                    else:\n",
    "                        if(not is_repeat(candidate, non_academic_subs)):\n",
    "                            l = [candidate, '0', '0', 'UNK']\n",
    "                            word_embed = get_word_embedding(candidate)\n",
    "                            cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                            l.append(cos_sim)\n",
    "                            l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                            if(len(non_academic_subs) < 2):\n",
    "                                non_academic_subs.append(l)\n",
    "            if(len(academic_subs) < 2 or len(non_academic_subs) < 2):\n",
    "                ppdb_candidates = db.get_ppdb2_candidates(lemma)\n",
    "                for t in ppdb_candidates:\n",
    "                    candidate = t[0]\n",
    "                    if(candidate in coca_list or candidate in nawl_list or candidate in academic_list):\n",
    "                        try:\n",
    "                            if(not is_repeat(candidate, academic_subs)):\n",
    "                                if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == candidate].iloc[0]['COCA-All']):\n",
    "                                    l = [candidate, '0', '0', 'UNK']\n",
    "                                    word_embed = get_word_embedding(candidate)\n",
    "                                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                    l.append(cos_sim)\n",
    "                                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                    if(len(academic_subs) < 2):\n",
    "                                        academic_subs.append(l)\n",
    "                        except:\n",
    "                            if(not is_repeat(candidate, non_academic_subs)):\n",
    "                                l = [candidate, '0', '0', 'UNK']\n",
    "                                word_embed = get_word_embedding(candidate)\n",
    "                                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                l.append(cos_sim)\n",
    "                                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                if(len(non_academic_subs) < 2):\n",
    "                                    non_academic_subs.append(l)\n",
    "                    else:\n",
    "                        if(not is_repeat(candidate, non_academic_subs)):\n",
    "                            l = [candidate, '0', '0', 'UNK']\n",
    "                            word_embed = get_word_embedding(candidate)\n",
    "                            cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                            l.append(cos_sim)\n",
    "                            l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                            if(len(non_academic_subs) < 2):\n",
    "                                non_academic_subs.append(l)\n",
    "            if(len(academic_subs) == 2 and len(non_academic_subs) == 2):\n",
    "                academic_subs = sorted(academic_subs, key=lambda x: int(x[1]), reverse=True)\n",
    "                non_academic_subs = sorted(non_academic_subs, key=lambda x: int(x[1]), reverse=True)\n",
    "                valid_subs = list()\n",
    "                valid_subs.extend(academic_subs)\n",
    "                valid_subs.extend(non_academic_subs)\n",
    "                # valid_subs : [[candidate, freq, is_non_academic, pos, cos_target, euclidean_distance], ....]\n",
    "                word_pairs_test[(token_id, wordform)] = valid_subs\n",
    "        else:\n",
    "            no_replacement += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_academic_count, with_replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_pairs_test), no_replacement, len(test_d), len(test_d)-no_replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = list()\n",
    "for p in tqdm_notebook(word_pairs_test):\n",
    "    for candidate_info in word_pairs_test[p]:\n",
    "        pos_tags.append(candidate_info[3])\n",
    "pos_tags = set(pos_tags)\n",
    "le_pos = dict()\n",
    "for i, v in enumerate(pos_tags):\n",
    "    le_pos[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_pairs_test.pkl', 'wb') as f:\n",
    "    pickle.dump(word_pairs_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = 1\n",
    "doc = ''\n",
    "\n",
    "for p in tqdm_notebook(word_pairs_test):\n",
    "    for candidate_info in word_pairs_test[p]:\n",
    "\n",
    "        is_non_academic = candidate_info[2]\n",
    "        if(is_non_academic == '0'):\n",
    "            freq = candidate_info[1]\n",
    "        elif(is_non_academic == '1'):\n",
    "            freq = ('-' + candidate_info[1]) # string concate to put it into the parser\n",
    "        target_value = freq\n",
    "        \n",
    "        lemma = candidate_info[0]\n",
    "\n",
    "        try:\n",
    "            freq_beatiful = beatiful_data_freq[lemma]\n",
    "        except:\n",
    "            freq_beatiful = 0\n",
    "        f_1 = feature2index['freq_beatiful']\n",
    "        \n",
    "        try:\n",
    "            freq_coca_general = allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All']\n",
    "        except:\n",
    "            freq_coca_general = 0\n",
    "        f_2 = feature2index['freq_coca_general']    \n",
    "        \n",
    "        try:\n",
    "            freq_acl = acl_freq[tuple(lemma.split())]\n",
    "        except:\n",
    "            freq_acl = 0\n",
    "        f_3 = feature2index['freq_acl']\n",
    "        \n",
    "        cos_target = candidate_info[4]\n",
    "        f_4 = feature2index['cos_target']\n",
    "        \n",
    "        euclidean_distance = candidate_info[5]\n",
    "        f_5 = feature2index['euclidean_distance']\n",
    "        \n",
    "        posMASC = candidate_info[3]\n",
    "        posMASC_le = le_pos[posMASC]\n",
    "        f_6 = feature2index['posMASC_le']\n",
    "        \n",
    "        # is_problematic\n",
    "        \n",
    "        word_length = len(lemma)\n",
    "        f_8 = feature2index['word_length']\n",
    "        \n",
    "        count_vowel = sum(list(map(lemma.lower().count, 'aeiou')))\n",
    "        f_9 = feature2index['count_vowel']\n",
    "        \n",
    "        doc += (str(target_value) + ' qid:' + str(qid) + ' ' + str(f_1) + ':' + str(freq_beatiful) + ' ' + str(f_2) + ':' + str(freq_coca_general) + ' ' + str(f_3) + ':' + str(freq_acl) + ' ' + str(f_4) + ':' + str(cos_target) + ' ' + str(f_5) + ':' + str(euclidean_distance) + ' ' + str(f_6) + ':' + str(posMASC_le) + ' ' + str(f_8) + ':' + str(word_length) + ' ' + str(f_9) + ':' + str(count_vowel) + '\\n')\n",
    "\n",
    "    qid += 1\n",
    "\n",
    "with open('paraphrase-test.txt', 'w') as f:\n",
    "    f.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_pairs_t), len(word_pairs_v), len(word_pairs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs_train = dict()\n",
    "for token_id in tqdm_notebook(train_d):\n",
    "    lemma = train_d[token_id]['lemma']\n",
    "    wordform = train_d[token_id]['wordform']\n",
    "    sentence_embed = get_sentence_embedding(train_d[token_id]['targetsentence'])\n",
    "    if(lemma not in coca_list or lemma not in nawl_list or lemma not in academic_list):\n",
    "        academic_subs = list()\n",
    "        non_academic_subs = list()\n",
    "        for subst in train_d[token_id]['substitutions']:\n",
    "            s = subst[0]\n",
    "            if(s in coca_list or s in nawl_list or s in academic_list):\n",
    "                try:\n",
    "                    if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == s].iloc[0]['COCA-All']):\n",
    "                        l = list(subst)\n",
    "                        pos = l.pop(1)\n",
    "                        l.append('0')\n",
    "                        l.append(pos)\n",
    "                        word_embed = get_word_embedding(s)\n",
    "                        cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                        l.append(cos_sim)\n",
    "                        l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                        if(len(academic_subs) < 2):\n",
    "                            academic_subs.append(l)\n",
    "                except:\n",
    "                    l = list(subst)\n",
    "                    pos = l.pop(1)\n",
    "                    l.append('1')\n",
    "                    l.append(pos)\n",
    "                    word_embed = get_word_embedding(s)\n",
    "                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                    l.append(cos_sim)\n",
    "                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                    if(len(non_academic_subs) < 2):\n",
    "                        non_academic_subs.append(l)\n",
    "            else:\n",
    "                l = list(subst)\n",
    "                pos = l.pop(1)\n",
    "                l.append('1')\n",
    "                l.append(pos)\n",
    "                word_embed = get_word_embedding(s)\n",
    "                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                l.append(cos_sim)\n",
    "                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                if(len(non_academic_subs) < 2):\n",
    "                    non_academic_subs.append(l)\n",
    "\n",
    "        if(academic_subs):\n",
    "            if(len(academic_subs) < 2 or len(non_academic_subs) < 2):\n",
    "                wordnet_candidates = db.get_wordnet_candidates(lemma)\n",
    "                for t in wordnet_candidates:\n",
    "                    candidate = t[0]\n",
    "                    if(candidate in coca_list or candidate in nawl_list or candidate in academic_list):\n",
    "                        try:\n",
    "                            if(not is_repeat(candidate, academic_subs)):\n",
    "                                if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == candidate].iloc[0]['COCA-All']):\n",
    "                                    l = [candidate, '0', '0', 'UNK']\n",
    "                                    word_embed = get_word_embedding(candidate)\n",
    "                                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                    l.append(cos_sim)\n",
    "                                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                    if(len(academic_subs) < 2):\n",
    "                                        academic_subs.append(l)\n",
    "                        except:\n",
    "                            if(not is_repeat(candidate, non_academic_subs)):\n",
    "                                l = [candidate, '0', '0', 'UNK']\n",
    "                                word_embed = get_word_embedding(candidate)\n",
    "                                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                l.append(cos_sim)\n",
    "                                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                if(len(non_academic_subs) < 2):\n",
    "                                    non_academic_subs.append(l)\n",
    "                    else:\n",
    "                        if(not is_repeat(candidate, non_academic_subs)):\n",
    "                            l = [candidate, '0', '0', 'UNK']\n",
    "                            word_embed = get_word_embedding(candidate)\n",
    "                            cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                            l.append(cos_sim)\n",
    "                            l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                            if(len(non_academic_subs) < 2):\n",
    "                                non_academic_subs.append(l)\n",
    "            if(len(academic_subs) < 2 or len(non_academic_subs) < 2):\n",
    "                ppdb_candidates = db.get_ppdb2_candidates(lemma)\n",
    "                for t in ppdb_candidates:\n",
    "                    candidate = t[0]\n",
    "                    if(candidate in coca_list or candidate in nawl_list or candidate in academic_list):\n",
    "                        try:\n",
    "                            if(not is_repeat(candidate, academic_subs)):\n",
    "                                if(allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All'] > allwords_df.loc[allwords_df['word'] == candidate].iloc[0]['COCA-All']):\n",
    "                                    l = [candidate, '0', '0', 'UNK']\n",
    "                                    word_embed = get_word_embedding(candidate)\n",
    "                                    cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                    l.append(cos_sim)\n",
    "                                    l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                    if(len(academic_subs) < 2):\n",
    "                                        academic_subs.append(l)\n",
    "                        except:\n",
    "                            if(not is_repeat(candidate, non_academic_subs)):\n",
    "                                l = [candidate, '0', '0', 'UNK']\n",
    "                                word_embed = get_word_embedding(candidate)\n",
    "                                cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                                l.append(cos_sim)\n",
    "                                l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                                if(len(non_academic_subs) < 2):\n",
    "                                    non_academic_subs.append(l)\n",
    "                    else:\n",
    "                        if(not is_repeat(candidate, non_academic_subs)):\n",
    "                            l = [candidate, '0', '0', 'UNK']\n",
    "                            word_embed = get_word_embedding(candidate)\n",
    "                            cos_sim = np.dot(sentence_embed, word_embed)/(np.sqrt(np.dot(sentence_embed, sentence_embed))*np.sqrt(np.dot(word_embed, word_embed)))\n",
    "                            l.append(cos_sim)\n",
    "                            l.append(np.linalg.norm(sentence_embed-word_embed))\n",
    "                            if(len(non_academic_subs) < 2):\n",
    "                                non_academic_subs.append(l)\n",
    "            if(len(academic_subs) == 2 and len(non_academic_subs) == 2):\n",
    "                academic_subs = sorted(academic_subs, key=lambda x: int(x[1]), reverse=True)\n",
    "                non_academic_subs = sorted(non_academic_subs, key=lambda x: int(x[1]), reverse=True)\n",
    "                valid_subs = list()\n",
    "                valid_subs.extend(academic_subs)\n",
    "                valid_subs.extend(non_academic_subs)\n",
    "                # valid_subs : [[candidate, freq, is_non_academic, pos, cos_target, euclidean_distance], ....]\n",
    "                word_pairs_train[(token_id, wordform)] = valid_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = list()\n",
    "for p in tqdm_notebook(word_pairs_train):\n",
    "    for candidate_info in word_pairs_train[p]:\n",
    "        pos_tags.append(candidate_info[3])\n",
    "pos_tags = set(pos_tags)\n",
    "le_pos = dict()\n",
    "for i, v in enumerate(pos_tags):\n",
    "    le_pos[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_pairs_train.pkl', 'wb') as f:\n",
    "    pickle.dump(word_pairs_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_pairs_train.pkl', 'rb') as f:\n",
    "    word_pairs_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_pairs_train)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = 1\n",
    "doc = ''\n",
    "\n",
    "for p in tqdm_notebook(word_pairs_train):\n",
    "    for candidate_info in word_pairs_train[p]:\n",
    "\n",
    "        is_non_academic = candidate_info[2]\n",
    "        if(is_non_academic == '0'):\n",
    "            freq = candidate_info[1]\n",
    "        elif(is_non_academic == '1'):\n",
    "            freq = ('-' + candidate_info[1]) # string concate to put it into the parser\n",
    "        target_value = freq\n",
    "        \n",
    "        lemma = candidate_info[0]\n",
    "\n",
    "        try:\n",
    "            freq_beatiful = beatiful_data_freq[lemma]\n",
    "        except:\n",
    "            freq_beatiful = 0\n",
    "        f_1 = feature2index['freq_beatiful']\n",
    "        \n",
    "        try:\n",
    "            freq_coca_general = allwords_df.loc[allwords_df['word'] == lemma].iloc[0]['COCA-All']\n",
    "        except:\n",
    "            freq_coca_general = 0\n",
    "        f_2 = feature2index['freq_coca_general']    \n",
    "        \n",
    "        try:\n",
    "            freq_acl = acl_freq[tuple(lemma.split())]\n",
    "        except:\n",
    "            freq_acl = 0\n",
    "        f_3 = feature2index['freq_acl']\n",
    "        \n",
    "        cos_target = candidate_info[4]\n",
    "        f_4 = feature2index['cos_target']\n",
    "        \n",
    "        euclidean_distance = candidate_info[5]\n",
    "        f_5 = feature2index['euclidean_distance']\n",
    "        \n",
    "        posMASC = candidate_info[3]\n",
    "        posMASC_le = le_pos[posMASC]\n",
    "        f_6 = feature2index['posMASC_le']\n",
    "        \n",
    "        # is_problematic\n",
    "        \n",
    "        word_length = len(lemma)\n",
    "        f_8 = feature2index['word_length']\n",
    "        \n",
    "        count_vowel = sum(list(map(lemma.lower().count, 'aeiou')))\n",
    "        f_9 = feature2index['count_vowel']\n",
    "        \n",
    "        doc += (str(target_value) + ' qid:' + str(qid) + ' ' + str(f_1) + ':' + str(freq_beatiful) + ' ' + str(f_2) + ':' + str(freq_coca_general) + ' ' + str(f_3) + ':' + str(freq_acl) + ' ' + str(f_4) + ':' + str(cos_target) + ' ' + str(f_5) + ':' + str(euclidean_distance) + ' ' + str(f_6) + ':' + str(posMASC_le) + ' ' + str(f_8) + ':' + str(word_length) + ' ' + str(f_9) + ':' + str(count_vowel) + '\\n')\n",
    "\n",
    "    qid += 1\n",
    "\n",
    "with open('paraphrase-train.txt', 'w') as f:\n",
    "    f.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
